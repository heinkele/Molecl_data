{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gradient-based feature extraction from event windows\n\nThis notebook implements a standalone pipeline that reloads the per-event windows saved by the segmentation workflow, detects the onset/offset of each event from the filtered trace, and integrates the corresponding baseline-corrected raw signal."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Workflow overview\n\n1. **Load event windows** from the JSON files generated during screening.\n2. **Pre-process the filtered trace** with a rolling mean and use its gradient to highlight transition points.\n3. **Locate event boundaries** by thresholding the gradient magnitude.\n4. **Reuse the stored baseline** for the window.\n5. **Integrate the raw trace** above the baseline between the detected bounds.\n6. **Persist derived features** for downstream analysis.\n7. **Perform quality checks** by plotting selected events."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom screening_sample_ssd import get_data_folders"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATA_PATH = Path('/Users/hugo/MOLECL/Molecl_data_H')  # Root directory or JSON file with event windows\nOUTPUT_PATH = Path('feature_metrics.csv')\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Expected event window data under {DATA_PATH}. Please update DATA_PATH before running the notebook.\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _contains_baseline_segment(path: Path) -> bool:\n    parts = [part.lower() for part in path.parts]\n    return any('baseline' in part for part in parts)\n\n\ndef discover_event_files(data_path: Path) -> List[Path]:\n    if data_path.is_file():\n        return [data_path]\n\n    if data_path.is_dir():\n        event_files: List[Path] = []\n        combined_here = data_path / 'combined_peaks_data.json'\n        if combined_here.exists():\n            event_files.append(combined_here)\n\n        candidate_dirs: List[Path] = []\n        try:\n            candidate_dirs.extend(Path(folder) for folder in get_data_folders(str(data_path)))\n        except Exception:\n            candidate_dirs.extend(path for path in data_path.iterdir() if path.is_dir())\n\n        seen_dirs = set()\n        for folder in candidate_dirs:\n            folder = Path(folder)\n            if folder in seen_dirs or not folder.exists():\n                continue\n            seen_dirs.add(folder)\n            if _contains_baseline_segment(folder):\n                continue\n\n            combined = folder / 'combined_peaks_data.json'\n            if combined.exists():\n                event_files.append(combined)\n\n            for json_file in sorted(folder.rglob('peaks_data.json')):\n                if not _contains_baseline_segment(json_file.parent):\n                    event_files.append(json_file)\n\n        if not event_files:\n            for json_file in sorted(data_path.rglob('peaks_data.json')):\n                if not _contains_baseline_segment(json_file.parent):\n                    event_files.append(json_file)\n\n        unique_files: List[Path] = []\n        seen_paths = set()\n        for json_file in event_files:\n            json_file = Path(json_file)\n            if not json_file.exists():\n                continue\n            resolved = json_file.resolve()\n            if resolved in seen_paths:\n                continue\n            seen_paths.add(resolved)\n            unique_files.append(json_file)\n\n        if unique_files:\n            return unique_files\n\n    raise FileNotFoundError(\n        f'No peak window JSON files found under {data_path}. Ensure the segmentation pipeline has generated peaks_data.json files.'\n    )\n\n\ndef load_peak_windows(event_files: Iterable[Path]) -> List[Dict]:\n    windows: List[Dict] = []\n    for json_path in event_files:\n        json_path = Path(json_path)\n        with json_path.open() as f:\n            payload = json.load(f)\n\n        def append_event(event: Dict, idx: int, batch_key: Optional[str] = None) -> None:\n            enriched = dict(event)\n            enriched.setdefault('event_index', idx)\n            if batch_key is not None:\n                enriched.setdefault('batch_id', batch_key)\n            enriched.setdefault('source_json', str(json_path))\n            enriched.setdefault('sample_folder', json_path.parent.name)\n            if 'source_file' not in enriched:\n                enriched['source_file'] = json_path.stem\n            enriched.setdefault('global_event_index', len(windows))\n            windows.append(enriched)\n\n        if isinstance(payload, list):\n            for idx, event in enumerate(payload):\n                if isinstance(event, dict):\n                    append_event(event, idx)\n        elif isinstance(payload, dict):\n            for batch_key, batch_events in payload.items():\n                if not isinstance(batch_events, Iterable):\n                    continue\n                for idx, event in enumerate(batch_events):\n                    if isinstance(event, dict):\n                        append_event(event, idx, batch_key=batch_key)\n        else:\n            raise ValueError(f'Unsupported JSON structure in {json_path}')\n\n    if not windows:\n        raise ValueError('No events found in the provided JSON files.')\n\n    return windows\n\n\nEVENT_FILES = discover_event_files(DATA_PATH)\nraw_events = load_peak_windows(EVENT_FILES)\nlen(EVENT_FILES), len(raw_events)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ensure_odd(window: int) -> int:\n    window = int(window)\n    return window + 1 if window % 2 == 0 else window\n\n\ndef smooth_signal(signal: np.ndarray, window: int = 11) -> np.ndarray:\n    if window <= 1:\n        return signal\n\n    window = ensure_odd(window)\n    series = pd.Series(signal)\n    smoothed = series.rolling(window, center=True, min_periods=1).mean().to_numpy()\n    return smoothed\n\n\ndef compute_gradients(filtered: np.ndarray, dt: float, window: int = 11) -> Tuple[np.ndarray, np.ndarray]:\n    smoothed = smooth_signal(filtered, window=window)\n    grad = np.gradient(smoothed, dt)\n    max_abs = np.max(np.abs(grad))\n    if max_abs == 0:\n        norm_grad = np.zeros_like(grad)\n    else:\n        norm_grad = grad / max_abs\n    norm_grad = smooth_signal(norm_grad, window=max(3, window // 2))\n    return grad, norm_grad\n\n\ndef mad(arr: np.ndarray) -> float:\n    median = np.median(arr)\n    return float(np.median(np.abs(arr - median)))\n\n\ndef gradient_threshold(norm_grad: np.ndarray, scale: float = 3.5) -> float:\n    noise_level = mad(norm_grad)\n    if noise_level == 0:\n        noise_level = np.std(norm_grad)\n    threshold = scale * noise_level if noise_level > 0 else 0.1\n    return float(np.clip(threshold, 0.05, 1.0))\n\n\ndef detect_bounds(norm_grad: np.ndarray, threshold: float, guard: int = 5, min_span: int = 10) -> Tuple[int, int]:\n    if threshold <= 0:\n        return 0, len(norm_grad) - 1\n\n    above = np.where(np.abs(norm_grad) >= threshold)[0]\n    if above.size == 0:\n        return 0, len(norm_grad) - 1\n\n    start = max(0, int(above[0]) - guard)\n    end = min(len(norm_grad) - 1, int(above[-1]) + guard)\n\n    if end - start < min_span:\n        center = int(above[above.size // 2])\n        half_span = max(min_span // 2, 1)\n        start = max(0, center - half_span)\n        end = min(len(norm_grad) - 1, center + half_span)\n\n    return start, end\n\n\ndef integrate_baseline_corrected(raw_signal: np.ndarray, baseline: float, start: int, end: int, dt: float) -> float:\n    segment = raw_signal[start : end + 1] - baseline\n    return float(np.trapz(segment, dx=dt))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_event_features(event: Dict, gradient_window: int = 21, threshold_scale: float = 3.5, guard: int = 10, min_span: int = 20) -> Dict:\n    raw, filtered, time_axis, dt, baseline = prepare_event_arrays(event)\n    _, norm_grad = compute_gradients(filtered, dt, window=gradient_window)\n    threshold = gradient_threshold(norm_grad, scale=threshold_scale)\n    start_idx, end_idx = detect_bounds(norm_grad, threshold, guard=guard, min_span=min_span)\n    area = integrate_baseline_corrected(raw, baseline, start_idx, end_idx, dt)\n\n    return {\n        'global_event_index': event.get('global_event_index'),\n        'event_index': event.get('event_index'),\n        'batch_id': event.get('batch_id'),\n        'sample_folder': event.get('sample_folder'),\n        'source_file': event.get('source_file'),\n        'source_json': event.get('source_json'),\n        't_start': time_axis[start_idx],\n        't_end': time_axis[end_idx],\n        'duration': (end_idx - start_idx) * dt,\n        'baseline': baseline,\n        'area_under_curve': area,\n        'start_idx': start_idx,\n        'end_idx': end_idx,\n        'threshold': threshold,\n        'gradient_window': gradient_window,\n        'threshold_scale': threshold_scale,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "event_features = [compute_event_features(event) for event in raw_events]\nfeatures_df = pd.DataFrame(event_features)\nfeatures_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "features_df.to_csv(OUTPUT_PATH, index=False)\nOUTPUT_PATH"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_event(event: Dict, features: Dict, gradient_window: int = 21):\n    raw, filtered, time_axis, dt, baseline = prepare_event_arrays(event)\n    _, norm_grad = compute_gradients(filtered, dt, window=gradient_window)\n\n    start_idx = int(features['start_idx'])\n    end_idx = int(features['end_idx'])\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n\n    axes[0].plot(time_axis, filtered, label='Filtered signal', color='tab:blue')\n    axes[0].axhline(baseline, color='tab:gray', linestyle='--', label='Baseline')\n    axes[0].axvspan(time_axis[start_idx], time_axis[end_idx], color='tab:orange', alpha=0.2, label='Event bounds')\n    axes[0].set_ylabel('Filtered (a.u.)')\n    axes[0].legend(loc='upper right')\n\n    axes[1].plot(time_axis, raw, label='Raw signal', color='tab:green')\n    axes[1].axhline(baseline, color='tab:gray', linestyle='--', label='Baseline')\n    axes[1].axvspan(time_axis[start_idx], time_axis[end_idx], color='tab:orange', alpha=0.2, label='Integrated window')\n    axes[1].set_ylabel('Raw (a.u.)')\n    axes[1].set_xlabel('Time (s)')\n    axes[1].legend(loc='upper right')\n\n    inset = axes[0].twinx()\n    inset.plot(time_axis, np.abs(norm_grad), color='tab:red', alpha=0.5, label='|Normalised gradient|')\n    inset.axhline(features['threshold'], color='tab:red', linestyle=':', label='Threshold')\n    inset.set_ylabel('Gradient magnitude')\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage: replace 0 with the index you wish to inspect\nexample_idx = 0\nplot_event(raw_events[example_idx], event_features[example_idx])"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}