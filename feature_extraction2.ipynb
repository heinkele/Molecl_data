{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Gradient-based feature extraction from event windows\n\nThis notebook implements a standalone pipeline that reloads the per-event windows saved by the segmentation workflow, detects the onset/offset of each event from the filtered trace, and integrates the corresponding baseline-corrected raw signal."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Workflow overview\n\n1. **Load event windows** from the JSON files generated during screening.\n2. **Pre-process the filtered trace** with a rolling mean and use its gradient to highlight transition points.\n3. **Locate event boundaries** by thresholding the gradient magnitude.\n4. **Reuse the stored baseline** for the window.\n5. **Integrate the raw trace** above the baseline between the detected bounds.\n6. **Persist derived features** for downstream analysis.\n7. **Perform quality checks** by plotting selected events."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Imports\nfrom __future__ import annotations\n\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Configuration\nDATA_PATH = Path('path/to/combined_peaks_data.json')  # Update with your data location\nOUTPUT_PATH = Path('feature_metrics.csv')\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Expected event window data at {DATA_PATH}. Please update DATA_PATH before running the notebook.\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_peak_windows(json_path: Path) -> List[Dict]:\n    # Load the stored event windows from a JSON file.\n    with json_path.open() as f:\n        payload = json.load(f)\n\n    windows: List[Dict] = []\n\n    if isinstance(payload, list):\n        for idx, event in enumerate(payload):\n            if isinstance(event, dict):\n                event.setdefault('event_index', idx)\n                windows.append(event)\n    elif isinstance(payload, dict):\n        for batch_key, batch_events in payload.items():\n            if not isinstance(batch_events, Iterable):\n                continue\n            for idx, event in enumerate(batch_events):\n                if not isinstance(event, dict):\n                    continue\n                enriched = dict(event)\n                enriched.setdefault('batch_id', batch_key)\n                enriched.setdefault('event_index', idx)\n                windows.append(enriched)\n    else:\n        raise ValueError('Unsupported JSON structure for peak windows.')\n\n    if not windows:\n        raise ValueError('No events found in the provided JSON file.')\n\n    return windows\n\nraw_events = load_peak_windows(DATA_PATH)\nlen(raw_events)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def ensure_odd(window: int) -> int:\n    window = int(window)\n    return window + 1 if window % 2 == 0 else window\n\n\ndef smooth_signal(signal: np.ndarray, window: int = 11) -> np.ndarray:\n    if window <= 1:\n        return signal\n\n    window = ensure_odd(window)\n    series = pd.Series(signal)\n    smoothed = series.rolling(window, center=True, min_periods=1).mean().to_numpy()\n    return smoothed\n\n\ndef compute_gradients(filtered: np.ndarray, dt: float, window: int = 11) -> Tuple[np.ndarray, np.ndarray]:\n    smoothed = smooth_signal(filtered, window=window)\n    grad = np.gradient(smoothed, dt)\n    max_abs = np.max(np.abs(grad))\n    if max_abs == 0:\n        norm_grad = np.zeros_like(grad)\n    else:\n        norm_grad = grad / max_abs\n    norm_grad = smooth_signal(norm_grad, window=max(3, window // 2))\n    return grad, norm_grad\n\n\ndef mad(arr: np.ndarray) -> float:\n    median = np.median(arr)\n    return float(np.median(np.abs(arr - median)))\n\n\ndef gradient_threshold(norm_grad: np.ndarray, scale: float = 3.5) -> float:\n    noise_level = mad(norm_grad)\n    if noise_level == 0:\n        noise_level = np.std(norm_grad)\n    threshold = scale * noise_level if noise_level > 0 else 0.1\n    return float(np.clip(threshold, 0.05, 1.0))\n\n\ndef detect_bounds(norm_grad: np.ndarray, threshold: float, guard: int = 5, min_span: int = 10) -> Tuple[int, int]:\n    if threshold <= 0:\n        return 0, len(norm_grad) - 1\n\n    above = np.where(np.abs(norm_grad) >= threshold)[0]\n    if above.size == 0:\n        return 0, len(norm_grad) - 1\n\n    start = max(0, int(above[0]) - guard)\n    end = min(len(norm_grad) - 1, int(above[-1]) + guard)\n\n    if end - start < min_span:\n        center = int(above[above.size // 2])\n        half_span = max(min_span // 2, 1)\n        start = max(0, center - half_span)\n        end = min(len(norm_grad) - 1, center + half_span)\n\n    return start, end\n\n\ndef integrate_baseline_corrected(raw_signal: np.ndarray, baseline: float, start: int, end: int, dt: float) -> float:\n    segment = raw_signal[start : end + 1] - baseline\n    return float(np.trapz(segment, dx=dt))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def prepare_event_arrays(event: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float, float]:\n    raw = np.asarray(event['raw_signal_not_norm'], dtype=float)\n    filtered = np.asarray(event['filtered_signal'], dtype=float)\n    dt = float(event['dt'])\n    t_start = float(event['t_start'])\n    baseline = float(event.get('baseline', 0.0))\n\n    if dt <= 0:\n        raise ValueError('Sampling interval dt must be positive.')\n\n    time_axis = t_start + np.arange(raw.size) * dt\n    return raw, filtered, time_axis, dt, baseline\n\n\ndef compute_event_features(event: Dict, gradient_window: int = 21, threshold_scale: float = 3.5, guard: int = 10, min_span: int = 20) -> Dict:\n    raw, filtered, time_axis, dt, baseline = prepare_event_arrays(event)\n    _, norm_grad = compute_gradients(filtered, dt, window=gradient_window)\n    threshold = gradient_threshold(norm_grad, scale=threshold_scale)\n    start_idx, end_idx = detect_bounds(norm_grad, threshold, guard=guard, min_span=min_span)\n    area = integrate_baseline_corrected(raw, baseline, start_idx, end_idx, dt)\n\n    return {\n        'event_index': event.get('event_index'),\n        'batch_id': event.get('batch_id'),\n        'source_file': event.get('source_file'),\n        't_start': time_axis[start_idx],\n        't_end': time_axis[end_idx],\n        'duration': (end_idx - start_idx) * dt,\n        'baseline': baseline,\n        'area_under_curve': area,\n        'start_idx': start_idx,\n        'end_idx': end_idx,\n        'threshold': threshold,\n        'gradient_window': gradient_window,\n        'threshold_scale': threshold_scale,\n    }"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "event_features = [compute_event_features(event) for event in raw_events]\nfeatures_df = pd.DataFrame(event_features)\nfeatures_df.head()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "features_df.to_csv(OUTPUT_PATH, index=False)\nOUTPUT_PATH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def plot_event(event: Dict, features: Dict, gradient_window: int = 21):\n    raw, filtered, time_axis, dt, baseline = prepare_event_arrays(event)\n    _, norm_grad = compute_gradients(filtered, dt, window=gradient_window)\n\n    start_idx = int(features['start_idx'])\n    end_idx = int(features['end_idx'])\n\n    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n\n    axes[0].plot(time_axis, filtered, label='Filtered signal', color='tab:blue')\n    axes[0].axhline(baseline, color='tab:gray', linestyle='--', label='Baseline')\n    axes[0].axvspan(time_axis[start_idx], time_axis[end_idx], color='tab:orange', alpha=0.2, label='Event bounds')\n    axes[0].set_ylabel('Filtered (a.u.)')\n    axes[0].legend(loc='upper right')\n\n    axes[1].plot(time_axis, raw, label='Raw signal', color='tab:green')\n    axes[1].axhline(baseline, color='tab:gray', linestyle='--', label='Baseline')\n    axes[1].axvspan(time_axis[start_idx], time_axis[end_idx], color='tab:orange', alpha=0.2, label='Integrated window')\n    axes[1].set_ylabel('Raw (a.u.)')\n    axes[1].set_xlabel('Time (s)')\n    axes[1].legend(loc='upper right')\n\n    inset = axes[0].twinx()\n    inset.plot(time_axis, np.abs(norm_grad), color='tab:red', alpha=0.5, label='|Normalised gradient|')\n    inset.axhline(features['threshold'], color='tab:red', linestyle=':', label='Threshold')\n    inset.set_ylabel('Gradient magnitude')\n\n    plt.tight_layout()\n    plt.show()\n\n# Example usage: replace 0 with the index you wish to inspect\nexample_idx = 0\nplot_event(raw_events[example_idx], event_features[example_idx])"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}