{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction from nanopore event traces\n",
    "\n",
    "This notebook loads `combined_peaks_data.json` files produced by the screening pipeline and computes the five baseline features (ΔI, standard deviation, skewness, kurtosis, and dwell time `t_off`) described in Wang *et al.* (2023). The resulting feature table will be reused for quick model prototyping (e.g. an SVM classifier).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow overview\n",
    "\n",
    "1. Configure the paths to the processed data folders that contain `combined_peaks_data.json`.\n",
    "2. Load every event trace and estimate the baseline and blockade regions.\n",
    "3. Compute ΔI, SD, skewness, kurtosis, and `t_off` for each event.\n",
    "4. Export the aggregated feature table for downstream modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data sources\n",
    "\n",
    "Set `DATA_ROOT` to the directory that contains one or more subfolders produced by `combine_peaks_data.py`. Each subfolder should contain a `combined_peaks_data.json`. If you prefer to point to specific files, populate `EXPLICIT_FILES` with their paths instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory that contains processed runs (update this to your environment)\n",
    "DATA_ROOT = Path('/path/to/processed/data/root')  # <-- change me\n",
    "\n",
    "# Optional: list explicit combined JSON files if they live in different roots\n",
    "EXPLICIT_FILES: List[str] = []  # e.g. ['/data/run1/combined_peaks_data.json']\n",
    "\n",
    "OUTPUT_DIR = Path('feature_tables')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EXPLICIT_FILES:\n",
    "    combined_files = [Path(p).expanduser().resolve() for p in EXPLICIT_FILES]\n",
    "elif DATA_ROOT.exists():\n",
    "    if DATA_ROOT.is_file() and DATA_ROOT.name == 'combined_peaks_data.json':\n",
    "        combined_files = [DATA_ROOT.expanduser().resolve()]\n",
    "    else:\n",
    "        combined_files = sorted(path.expanduser().resolve() for path in DATA_ROOT.rglob('combined_peaks_data.json'))\n",
    "else:\n",
    "    combined_files = []\n",
    "    print('Update DATA_ROOT or EXPLICIT_FILES with the location of combined_peaks_data.json files.')\n",
    "\n",
    "combined_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper utilities\n",
    "\n",
    "The functions below load events, identify the blockade portion of each trace, and compute the five requested features. The baseline is estimated from the high-current samples (outside the blockade), whereas the blockade level is taken from the low-current samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events(json_path: Path) -> List[Dict]:\n",
    "    \"\"\"Load the list of events from a combined peaks JSON file.\"\"\"\n",
    "    with open(json_path, 'r') as fh:\n",
    "        events = json.load(fh)\n",
    "    for event in events:\n",
    "        if 'source_file' not in event:\n",
    "            event['source_file'] = json_path.stem\n",
    "    return events\n",
    "\n",
    "\n",
    "def _safe_median(values: np.ndarray) -> float:\n",
    "    values = values[np.isfinite(values)]\n",
    "    if values.size == 0:\n",
    "        return float('nan')\n",
    "    return float(np.median(values))\n",
    "\n",
    "\n",
    "def _event_masks(norm_signal: np.ndarray, fraction: float = 0.25) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Return boolean masks that delineate blockade vs baseline samples.\"\"\"\n",
    "    sorted_norm = np.sort(norm_signal[np.isfinite(norm_signal)])\n",
    "    if sorted_norm.size == 0:\n",
    "        mask = np.zeros_like(norm_signal, dtype=bool)\n",
    "        return {\n",
    "            'event_mask': mask,\n",
    "            'baseline_mask': ~mask,\n",
    "            'baseline_level': float('nan'),\n",
    "            'blockade_level': float('nan'),\n",
    "            'delta_norm': float('nan'),\n",
    "            'threshold': float('nan'),\n",
    "        }\n",
    "\n",
    "    top_k = max(int(np.ceil(sorted_norm.size * 0.2)), 1)\n",
    "    baseline_level = float(np.median(sorted_norm[-top_k:]))\n",
    "    blockade_level = float(np.median(sorted_norm[:top_k]))\n",
    "    delta_norm = baseline_level - blockade_level\n",
    "\n",
    "    if delta_norm <= 0:\n",
    "        delta_norm = baseline_level - float(np.min(sorted_norm))\n",
    "\n",
    "    threshold = baseline_level - fraction * delta_norm if delta_norm > 0 else baseline_level - 1e-3\n",
    "    applied_threshold = threshold\n",
    "    event_mask = norm_signal <= applied_threshold\n",
    "\n",
    "    if not np.any(event_mask):\n",
    "        fallback_threshold = baseline_level - 0.1 * max(abs(delta_norm), abs(baseline_level) * 0.05)\n",
    "        event_mask = norm_signal <= fallback_threshold\n",
    "        applied_threshold = fallback_threshold\n",
    "\n",
    "    if not np.any(event_mask):\n",
    "        applied_threshold = baseline_level\n",
    "        event_mask = norm_signal < baseline_level\n",
    "\n",
    "    baseline_mask = ~event_mask\n",
    "\n",
    "    return {\n",
    "        'event_mask': event_mask,\n",
    "        'baseline_mask': baseline_mask,\n",
    "        'baseline_level': baseline_level,\n",
    "        'blockade_level': blockade_level,\n",
    "        'delta_norm': delta_norm,\n",
    "        'threshold': float(applied_threshold),\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_signals(event: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return normalized and absolute signals with NaNs removed.\"\"\"\n",
    "    raw_signal = event.get('raw_signal') or event.get('norm_signal')\n",
    "    if raw_signal is None:\n",
    "        raise KeyError('Event is missing a normalized signal (\"raw_signal\" or \"norm_signal\").')\n",
    "\n",
    "    raw_signal = np.asarray(raw_signal, dtype=float)\n",
    "    raw_signal_abs = np.asarray(event['raw_signal_not_norm'], dtype=float)\n",
    "\n",
    "    n_samples = min(raw_signal.size, raw_signal_abs.size)\n",
    "    raw_signal = raw_signal[:n_samples]\n",
    "    raw_signal_abs = raw_signal_abs[:n_samples]\n",
    "\n",
    "    valid = np.isfinite(raw_signal) & np.isfinite(raw_signal_abs)\n",
    "    return raw_signal[valid], raw_signal_abs[valid]\n",
    "\n",
    "\n",
    "def _event_time_axis(event: Dict, n_samples: int) -> Tuple[np.ndarray, str]:\n",
    "    \"\"\"Construct a time axis suitable for plotting the event.\"\"\"\n",
    "    dt = float(event.get('dt', np.nan))\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return np.arange(n_samples, dtype=float), 'Sample index'\n",
    "    axis = np.arange(n_samples, dtype=float) * dt\n",
    "    return axis, f'Time (dt={dt:g})'\n",
    "\n",
    "\n",
    "def _safe_std(values: np.ndarray) -> float:\n",
    "    if values.size < 2:\n",
    "        return float('nan')\n",
    "    return float(np.std(values, ddof=1))\n",
    "\n",
    "\n",
    "def _safe_skew(values: np.ndarray) -> float:\n",
    "    if values.size < 3:\n",
    "        return float('nan')\n",
    "    return float(skew(values, bias=False))\n",
    "\n",
    "\n",
    "def _safe_kurtosis(values: np.ndarray) -> float:\n",
    "    if values.size < 4:\n",
    "        return float('nan')\n",
    "    return float(kurtosis(values, fisher=True, bias=False))\n",
    "\n",
    "\n",
    "def compute_event_features(event: Dict, *, sample_id: Optional[str] = None, source_path: Optional[Path] = None) -> Dict:\n",
    "    raw_signal, raw_signal_abs = _extract_signals(event)\n",
    "\n",
    "    masks = _event_masks(raw_signal)\n",
    "    event_mask = masks['event_mask']\n",
    "    baseline_mask = masks['baseline_mask']\n",
    "\n",
    "    if np.any(baseline_mask):\n",
    "        baseline_current = _safe_median(raw_signal_abs[baseline_mask])\n",
    "    else:\n",
    "        baseline_current = _safe_median(raw_signal_abs)\n",
    "\n",
    "    if np.any(event_mask):\n",
    "        blockade_current = _safe_median(raw_signal_abs[event_mask])\n",
    "        drop_series = baseline_current - raw_signal_abs[event_mask]\n",
    "    else:\n",
    "        blockade_current = _safe_median(raw_signal_abs)\n",
    "        drop_series = baseline_current - raw_signal_abs\n",
    "\n",
    "    delta_I = float(baseline_current - blockade_current)\n",
    "    sd_drop = _safe_std(drop_series)\n",
    "    skew_drop = _safe_skew(drop_series)\n",
    "    kurt_drop = _safe_kurtosis(drop_series)\n",
    "\n",
    "    dt = float(event.get('dt', np.nan))\n",
    "    if np.isnan(dt) or dt == 0:\n",
    "        t_off = float(event.get('t_end', np.nan) - event.get('t_start', np.nan))\n",
    "    else:\n",
    "        if np.any(event_mask):\n",
    "            event_indices = np.where(event_mask)[0]\n",
    "            t_off = float((event_indices[-1] - event_indices[0] + 1) * dt)\n",
    "        else:\n",
    "            t_off = float(raw_signal.size * dt)\n",
    "\n",
    "    feature_record = {\n",
    "        'sample_id': sample_id if sample_id is not None else (source_path.parent.name if source_path else None),\n",
    "        'source_file': event.get('source_file', source_path.stem if source_path else None),\n",
    "        'peak_index': event.get('peak_index'),\n",
    "        'delta_I': delta_I,\n",
    "        'sd': sd_drop,\n",
    "        'skew': skew_drop,\n",
    "        'kurtosis': kurt_drop,\n",
    "        't_off': t_off,\n",
    "        'baseline_current': baseline_current,\n",
    "        'blocked_current': blockade_current,\n",
    "        'n_event_samples': int(np.sum(event_mask)),\n",
    "        'total_samples': int(raw_signal.size),\n",
    "    }\n",
    "\n",
    "    if source_path is not None:\n",
    "        feature_record['combined_file'] = str(source_path)\n",
    "\n",
    "    if 'snr_db' in event:\n",
    "        feature_record['snr_db'] = event['snr_db']\n",
    "\n",
    "    return feature_record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from every combined file\n",
    "\n",
    "Run the cell below after configuring the paths. A feature dictionary is created for every event and aggregated into a single DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_records: List[Dict] = []\n",
    "\n",
    "for combined_path in combined_files:\n",
    "    events = load_events(combined_path)\n",
    "    sample_id = combined_path.parent.name\n",
    "    for event in events:\n",
    "        try:\n",
    "            feature_records.append(\n",
    "                compute_event_features(event, sample_id=sample_id, source_path=combined_path)\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            print(f'Failed to process event from {combined_path}: {exc}')\n",
    "\n",
    "features_df = pd.DataFrame(feature_records)\n",
    "print(f'Loaded {len(feature_records)} events from {len(combined_files)} combined files.')\n",
    "features_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-event inspection\n",
    "\n",
    "Visualise a representative event to confirm the `_event_masks` segmentation. The plot overlays baseline and blockade samples, the derived threshold, and the `t_start`/`t_end` markers inferred from the mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise baseline vs blockade segmentation for a single event\n",
    "if not combined_files:\n",
    "    print('No combined_peaks_data.json files were discovered. Update DATA_ROOT or EXPLICIT_FILES to plot an event.')\n",
    "else:\n",
    "    example_path = combined_files[0]\n",
    "    events = load_events(example_path)\n",
    "\n",
    "    if not events:\n",
    "        print(f'No events found in {example_path}.')\n",
    "    else:\n",
    "        example_event = events[0]\n",
    "        try:\n",
    "            norm_signal, _ = _extract_signals(example_event)\n",
    "        except KeyError as exc:\n",
    "            print(f'Unable to read event signals: {exc}')\n",
    "        else:\n",
    "            masks = _event_masks(norm_signal)\n",
    "            time_axis, time_label = _event_time_axis(example_event, norm_signal.size)\n",
    "            event_indices = np.where(masks['event_mask'])[0]\n",
    "\n",
    "            sample_name = example_event.get('sample_id') or example_path.parent.name\n",
    "            event_label = example_event.get('peak_index', 'unknown')\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "            ax.plot(time_axis, norm_signal, color='tab:blue', linewidth=1.2, label='Normalized signal')\n",
    "\n",
    "            if np.any(masks['baseline_mask']):\n",
    "                ax.scatter(\n",
    "                    time_axis[masks['baseline_mask']],\n",
    "                    norm_signal[masks['baseline_mask']],\n",
    "                    color='tab:green',\n",
    "                    s=12,\n",
    "                    alpha=0.7,\n",
    "                    label='Baseline samples',\n",
    "                )\n",
    "\n",
    "            if np.any(masks['event_mask']):\n",
    "                ax.scatter(\n",
    "                    time_axis[masks['event_mask']],\n",
    "                    norm_signal[masks['event_mask']],\n",
    "                    color='tab:orange',\n",
    "                    s=12,\n",
    "                    alpha=0.9,\n",
    "                    label='Blockade samples',\n",
    "                )\n",
    "\n",
    "            ax.axhline(masks['baseline_level'], color='tab:green', linestyle='--', linewidth=1, label=f\"Baseline level ({masks['baseline_level']:.3g})\")\n",
    "            ax.axhline(masks['blockade_level'], color='tab:orange', linestyle='--', linewidth=1, label=f\"Blockade level ({masks['blockade_level']:.3g})\")\n",
    "\n",
    "            if np.isfinite(masks['threshold']):\n",
    "                ax.axhline(masks['threshold'], color='tab:red', linestyle=':', linewidth=1.2, label=f\"Threshold ({masks['threshold']:.3g})\")\n",
    "\n",
    "            if event_indices.size:\n",
    "                start_time = time_axis[event_indices[0]]\n",
    "                end_time = time_axis[event_indices[-1]]\n",
    "                ax.axvline(start_time, color='black', linestyle='--', linewidth=1, label='t_start')\n",
    "                ax.axvline(end_time, color='black', linestyle='-.', linewidth=1, label='t_end')\n",
    "\n",
    "            ax.set_xlabel(time_label)\n",
    "            ax.set_ylabel('Normalized current')\n",
    "            ax.set_title(f'Sample: {sample_name} — peak {event_label}')\n",
    "\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            legend = dict(zip(labels, handles))\n",
    "            ax.legend(legend.values(), legend.keys(), loc='best', frameon=True)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature distributions\n",
    "\n",
    "Use histograms and scatter plots to inspect the spread of the handcrafted features and how different samples occupy feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograms / KDE plots for the five features\n",
    "if features_df.empty:\n",
    "    print('The feature table is empty. Run the extraction cell above to populate `features_df`.')\n",
    "else:\n",
    "    columns = globals().get('feature_columns', ['delta_I', 'sd', 'skew', 'kurtosis', 't_off'])\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(4 * len(columns), 3.5), constrained_layout=True)\n",
    "    if len(columns) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, column in zip(axes, columns):\n",
    "        column_data = features_df[column].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if column_data.empty:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(column)\n",
    "            ax.set_xlabel(column)\n",
    "            ax.set_ylabel('Count')\n",
    "            continue\n",
    "        sns.histplot(column_data, bins=30, kde=True, ax=ax, color='tab:blue', edgecolor='white', linewidth=0.5)\n",
    "        ax.set_title(column)\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scatter plot to inspect ΔI vs. t_off coloured by sample\n",
    "if features_df.empty:\n",
    "    print('The feature table is empty. Run the extraction cell above to populate `features_df`.')\n",
    "else:\n",
    "    scatter_df = features_df[['delta_I', 't_off', 'sample_id']].copy()\n",
    "    scatter_df[['delta_I', 't_off']] = scatter_df[['delta_I', 't_off']].replace([np.inf, -np.inf], np.nan)\n",
    "    scatter_df = scatter_df.dropna(subset=['delta_I', 't_off'])\n",
    "\n",
    "    if scatter_df.empty:\n",
    "        print('No finite ΔI / t_off values are available for plotting.')\n",
    "    else:\n",
    "        scatter_df['sample_plot_label'] = scatter_df['sample_id'].fillna('Unknown').astype(str)\n",
    "        counts = scatter_df['sample_plot_label'].value_counts()\n",
    "        if counts.size > 10:\n",
    "            top_labels = counts.index[:9]\n",
    "            scatter_df['sample_plot_label'] = scatter_df['sample_plot_label'].where(scatter_df['sample_plot_label'].isin(top_labels), 'Other')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        sns.scatterplot(\n",
    "            data=scatter_df,\n",
    "            x='delta_I',\n",
    "            y='t_off',\n",
    "            hue='sample_plot_label',\n",
    "            palette='tab10',\n",
    "            s=45,\n",
    "            edgecolor='white',\n",
    "            linewidth=0.5,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xlabel('ΔI (baseline − blockade)')\n",
    "        ax.set_ylabel('t_off (dwell time)')\n",
    "        ax.set_title('ΔI vs. t_off by sample')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(title='Sample', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw vs. normalized traces\n",
    "\n",
    "Compare several events to verify that the normalisation preserves event structure and removes slow drift.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display a few raw and normalized traces side-by-side\n",
    "max_examples = 3\n",
    "trace_examples = []\n",
    "\n",
    "for combined_path in combined_files:\n",
    "    events = load_events(combined_path)\n",
    "    for event in events:\n",
    "        try:\n",
    "            norm_signal, raw_signal_abs = _extract_signals(event)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        if norm_signal.size == 0:\n",
    "            continue\n",
    "\n",
    "        time_axis, time_label = _event_time_axis(event, norm_signal.size)\n",
    "        label = f\"{combined_path.parent.name} — peak {event.get('peak_index', 'unknown')}\"\n",
    "        trace_examples.append((label, time_axis, time_label, raw_signal_abs, norm_signal))\n",
    "\n",
    "        if len(trace_examples) >= max_examples:\n",
    "            break\n",
    "    if len(trace_examples) >= max_examples:\n",
    "        break\n",
    "\n",
    "if not trace_examples:\n",
    "    print('No events available to plot. Update DATA_ROOT or EXPLICIT_FILES and re-run the extraction cell.')\n",
    "else:\n",
    "    fig, axes = plt.subplots(len(trace_examples), 2, figsize=(12, 3.2 * len(trace_examples)), sharex='col')\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for row, (label, time_axis, time_label, raw_abs, norm_sig) in enumerate(trace_examples):\n",
    "        raw_ax, norm_ax = axes[row, 0], axes[row, 1]\n",
    "\n",
    "        raw_ax.plot(time_axis, raw_abs, color='tab:blue', linewidth=1.0)\n",
    "        raw_ax.set_title(f'{label} — raw')\n",
    "        raw_ax.set_ylabel('Current (raw units)')\n",
    "        raw_ax.grid(True, alpha=0.3)\n",
    "\n",
    "        norm_ax.plot(time_axis, norm_sig, color='tab:orange', linewidth=1.0)\n",
    "        norm_ax.set_title(f'{label} — normalized')\n",
    "        norm_ax.set_ylabel('Normalized current')\n",
    "        norm_ax.grid(True, alpha=0.3)\n",
    "\n",
    "        raw_ax.set_xlabel(time_label)\n",
    "        norm_ax.set_xlabel(time_label)\n",
    "\n",
    "    fig.suptitle('Raw vs. normalized event traces', fontsize=14, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect summary statistics\n",
    "\n",
    "The table below provides a quick sanity check for the magnitude of each feature. Adjust the baseline detection heuristics above if the distributions look suspicious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['delta_I', 'sd', 'skew', 'kurtosis', 't_off']\n",
    "\n",
    "if not features_df.empty:\n",
    "    display(features_df[feature_columns].describe())\n",
    "else:\n",
    "    print('Feature table is empty. Verify DATA_ROOT/EXPLICIT_FILES before continuing.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the feature table\n",
    "\n",
    "Export the features to CSV so that other notebooks (e.g. model fitting) can consume them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUT_DIR / 'event_features.csv'\n",
    "\n",
    "if not features_df.empty:\n",
    "    features_df.to_csv(output_path, index=False)\n",
    "    print(f'Saved {len(features_df)} feature rows to {output_path}')\n",
    "else:\n",
    "    print('Skipped saving because the feature table is empty.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: map samples to class labels\n",
    "\n",
    "Create a dictionary that maps folder names (or `sample_id`) to the DNA classes you want to predict. Uncomment and adapt the cell below when preparing the modeling notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE_LABEL_MAP = {\n",
    "#     '500mV_100bp_1ngmkl_10MHz_boost_240830164444': '100bp',\n",
    "#     '500mV_200bp_1ngmkl_10MHz_boost_240830165006': '200bp',\n",
    "# }\n",
    "#\n",
    "# if not features_df.empty:\n",
    "#     features_df['label'] = features_df['sample_id'].map(SAMPLE_LABEL_MAP)\n",
    "#     display(features_df[['sample_id', 'label']].drop_duplicates())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}