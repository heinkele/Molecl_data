{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature extraction from nanopore event traces\n",
        "\n",
        "This notebook loads `combined_peaks_data.json` files produced by the screening pipeline and computes the five baseline features (\u0394I, standard deviation, skewness, kurtosis, and dwell time `t_off`) described in Wang *et al.* (2023). The resulting feature table will be reused for quick model prototyping (e.g. an SVM classifier).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow overview\n",
        "\n",
        "1. Configure the paths to the processed data folders that contain `combined_peaks_data.json`.\n",
        "2. Load every event trace and estimate the baseline and blockade regions.\n",
        "3. Compute \u0394I, SD, skewness, kurtosis, and `t_off` for each event.\n",
        "4. Export the aggregated feature table for downstream modeling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Dict, Iterable, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import kurtosis, skew\n",
        "\n",
        "pd.set_option('display.max_columns', 20)\n",
        "pd.set_option('display.width', 120)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure data sources\n",
        "\n",
        "Set `DATA_ROOT` to the directory that contains one or more subfolders produced by `combine_peaks_data.py`. Each subfolder should contain a `combined_peaks_data.json`. If you prefer to point to specific files, populate `EXPLICIT_FILES` with their paths instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the directory that contains processed runs (update this to your environment)\n",
        "DATA_ROOT = Path('/path/to/processed/data/root')  # <-- change me\n",
        "\n",
        "# Optional: list explicit combined JSON files if they live in different roots\n",
        "EXPLICIT_FILES: List[str] = []  # e.g. ['/data/run1/combined_peaks_data.json']\n",
        "\n",
        "OUTPUT_DIR = Path('feature_tables')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if EXPLICIT_FILES:\n",
        "    combined_files = [Path(p).expanduser().resolve() for p in EXPLICIT_FILES]\n",
        "elif DATA_ROOT.exists():\n",
        "    if DATA_ROOT.is_file() and DATA_ROOT.name == 'combined_peaks_data.json':\n",
        "        combined_files = [DATA_ROOT.expanduser().resolve()]\n",
        "    else:\n",
        "        combined_files = sorted(path.expanduser().resolve() for path in DATA_ROOT.rglob('combined_peaks_data.json'))\n",
        "else:\n",
        "    combined_files = []\n",
        "    print('Update DATA_ROOT or EXPLICIT_FILES with the location of combined_peaks_data.json files.')\n",
        "\n",
        "combined_files\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper utilities\n",
        "\n",
        "The functions below load events, identify the blockade portion of each trace, and compute the five requested features. The baseline is estimated from the high-current samples (outside the blockade), whereas the blockade level is taken from the low-current samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_events(json_path: Path) -> List[Dict]:\n",
        "    \"\"\"Load the list of events from a combined peaks JSON file.\"\"\"\n",
        "    with open(json_path, 'r') as fh:\n",
        "        events = json.load(fh)\n",
        "    for event in events:\n",
        "        if 'source_file' not in event:\n",
        "            event['source_file'] = json_path.stem\n",
        "    return events\n",
        "\n",
        "\n",
        "def _safe_median(values: np.ndarray) -> float:\n",
        "    values = values[np.isfinite(values)]\n",
        "    if values.size == 0:\n",
        "        return float('nan')\n",
        "    return float(np.median(values))\n",
        "\n",
        "\n",
        "def _event_masks(norm_signal: np.ndarray, fraction: float = 0.25) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Return boolean masks that delineate blockade vs baseline samples.\"\"\"\n",
        "    sorted_norm = np.sort(norm_signal[np.isfinite(norm_signal)])\n",
        "    if sorted_norm.size == 0:\n",
        "        mask = np.zeros_like(norm_signal, dtype=bool)\n",
        "        return {\n",
        "            'event_mask': mask,\n",
        "            'baseline_mask': ~mask,\n",
        "            'baseline_level': float('nan'),\n",
        "            'blockade_level': float('nan'),\n",
        "            'delta_norm': float('nan'),\n",
        "        }\n",
        "\n",
        "    top_k = max(int(np.ceil(sorted_norm.size * 0.2)), 1)\n",
        "    baseline_level = float(np.median(sorted_norm[-top_k:]))\n",
        "    blockade_level = float(np.median(sorted_norm[:top_k]))\n",
        "    delta_norm = baseline_level - blockade_level\n",
        "\n",
        "    if delta_norm <= 0:\n",
        "        delta_norm = baseline_level - float(np.min(sorted_norm))\n",
        "\n",
        "    threshold = baseline_level - fraction * delta_norm if delta_norm > 0 else baseline_level - 1e-3\n",
        "    event_mask = norm_signal <= threshold\n",
        "\n",
        "    if not np.any(event_mask):\n",
        "        fallback_threshold = baseline_level - 0.1 * max(abs(delta_norm), abs(baseline_level) * 0.05)\n",
        "        event_mask = norm_signal <= fallback_threshold\n",
        "\n",
        "    if not np.any(event_mask):\n",
        "        event_mask = norm_signal < baseline_level\n",
        "\n",
        "    baseline_mask = ~event_mask\n",
        "\n",
        "    return {\n",
        "        'event_mask': event_mask,\n",
        "        'baseline_mask': baseline_mask,\n",
        "        'baseline_level': baseline_level,\n",
        "        'blockade_level': blockade_level,\n",
        "        'delta_norm': delta_norm,\n",
        "    }\n",
        "\n",
        "\n",
        "def _safe_std(values: np.ndarray) -> float:\n",
        "    if values.size < 2:\n",
        "        return float('nan')\n",
        "    return float(np.std(values, ddof=1))\n",
        "\n",
        "\n",
        "def _safe_skew(values: np.ndarray) -> float:\n",
        "    if values.size < 3:\n",
        "        return float('nan')\n",
        "    return float(skew(values, bias=False))\n",
        "\n",
        "\n",
        "def _safe_kurtosis(values: np.ndarray) -> float:\n",
        "    if values.size < 4:\n",
        "        return float('nan')\n",
        "    return float(kurtosis(values, fisher=True, bias=False))\n",
        "\n",
        "\n",
        "def compute_event_features(event: Dict, *, sample_id: Optional[str] = None, source_path: Optional[Path] = None) -> Dict:\n",
        "    raw_signal = event.get('raw_signal') or event.get('norm_signal')\n",
        "    if raw_signal is None:\n",
        "        raise KeyError('Event is missing a normalized signal (\"raw_signal\" or \"norm_signal\").')\n",
        "\n",
        "    raw_signal = np.asarray(raw_signal, dtype=float)\n",
        "    raw_signal_abs = np.asarray(event['raw_signal_not_norm'], dtype=float)\n",
        "\n",
        "    n_samples = min(raw_signal.size, raw_signal_abs.size)\n",
        "    raw_signal = raw_signal[:n_samples]\n",
        "    raw_signal_abs = raw_signal_abs[:n_samples]\n",
        "\n",
        "    valid = np.isfinite(raw_signal) & np.isfinite(raw_signal_abs)\n",
        "    raw_signal = raw_signal[valid]\n",
        "    raw_signal_abs = raw_signal_abs[valid]\n",
        "\n",
        "    masks = _event_masks(raw_signal)\n",
        "    event_mask = masks['event_mask']\n",
        "    baseline_mask = masks['baseline_mask']\n",
        "\n",
        "    if np.any(baseline_mask):\n",
        "        baseline_current = _safe_median(raw_signal_abs[baseline_mask])\n",
        "    else:\n",
        "        baseline_current = _safe_median(raw_signal_abs)\n",
        "\n",
        "    if np.any(event_mask):\n",
        "        blockade_current = _safe_median(raw_signal_abs[event_mask])\n",
        "        drop_series = baseline_current - raw_signal_abs[event_mask]\n",
        "    else:\n",
        "        blockade_current = _safe_median(raw_signal_abs)\n",
        "        drop_series = baseline_current - raw_signal_abs\n",
        "\n",
        "    delta_I = float(baseline_current - blockade_current)\n",
        "    sd_drop = _safe_std(drop_series)\n",
        "    skew_drop = _safe_skew(drop_series)\n",
        "    kurt_drop = _safe_kurtosis(drop_series)\n",
        "\n",
        "    dt = float(event.get('dt', np.nan))\n",
        "    if np.isnan(dt) or dt == 0:\n",
        "        t_off = float(event.get('t_end', np.nan) - event.get('t_start', np.nan))\n",
        "    else:\n",
        "        if np.any(event_mask):\n",
        "            event_indices = np.where(event_mask)[0]\n",
        "            t_off = float((event_indices[-1] - event_indices[0] + 1) * dt)\n",
        "        else:\n",
        "            t_off = float(raw_signal.size * dt)\n",
        "\n",
        "    feature_record = {\n",
        "        'sample_id': sample_id if sample_id is not None else (source_path.parent.name if source_path else None),\n",
        "        'source_file': event.get('source_file', source_path.stem if source_path else None),\n",
        "        'peak_index': event.get('peak_index'),\n",
        "        'delta_I': delta_I,\n",
        "        'sd': sd_drop,\n",
        "        'skew': skew_drop,\n",
        "        'kurtosis': kurt_drop,\n",
        "        't_off': t_off,\n",
        "        'baseline_current': baseline_current,\n",
        "        'blocked_current': blockade_current,\n",
        "        'n_event_samples': int(np.sum(event_mask)),\n",
        "        'total_samples': int(raw_signal.size),\n",
        "    }\n",
        "\n",
        "    if source_path is not None:\n",
        "        feature_record['combined_file'] = str(source_path)\n",
        "\n",
        "    if 'snr_db' in event:\n",
        "        feature_record['snr_db'] = event['snr_db']\n",
        "\n",
        "    return feature_record\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract features from every combined file\n",
        "\n",
        "Run the cell below after configuring the paths. A feature dictionary is created for every event and aggregated into a single DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_records: List[Dict] = []\n",
        "\n",
        "for combined_path in combined_files:\n",
        "    events = load_events(combined_path)\n",
        "    sample_id = combined_path.parent.name\n",
        "    for event in events:\n",
        "        try:\n",
        "            feature_records.append(\n",
        "                compute_event_features(event, sample_id=sample_id, source_path=combined_path)\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            print(f'Failed to process event from {combined_path}: {exc}')\n",
        "\n",
        "features_df = pd.DataFrame(feature_records)\n",
        "print(f'Loaded {len(feature_records)} events from {len(combined_files)} combined files.')\n",
        "features_df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect summary statistics\n",
        "\n",
        "The table below provides a quick sanity check for the magnitude of each feature. Adjust the baseline detection heuristics above if the distributions look suspicious.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_columns = ['delta_I', 'sd', 'skew', 'kurtosis', 't_off']\n",
        "\n",
        "if not features_df.empty:\n",
        "    display(features_df[feature_columns].describe())\n",
        "else:\n",
        "    print('Feature table is empty. Verify DATA_ROOT/EXPLICIT_FILES before continuing.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persist the feature table\n",
        "\n",
        "Export the features to CSV so that other notebooks (e.g. model fitting) can consume them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_path = OUTPUT_DIR / 'event_features.csv'\n",
        "\n",
        "if not features_df.empty:\n",
        "    features_df.to_csv(output_path, index=False)\n",
        "    print(f'Saved {len(features_df)} feature rows to {output_path}')\n",
        "else:\n",
        "    print('Skipped saving because the feature table is empty.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: map samples to class labels\n",
        "\n",
        "Create a dictionary that maps folder names (or `sample_id`) to the DNA classes you want to predict. Uncomment and adapt the cell below when preparing the modeling notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAMPLE_LABEL_MAP = {\n",
        "#     '500mV_100bp_1ngmkl_10MHz_boost_240830164444': '100bp',\n",
        "#     '500mV_200bp_1ngmkl_10MHz_boost_240830165006': '200bp',\n",
        "# }\n",
        "#\n",
        "# if not features_df.empty:\n",
        "#     features_df['label'] = features_df['sample_id'].map(SAMPLE_LABEL_MAP)\n",
        "#     display(features_df[['sample_id', 'label']].drop_duplicates())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}